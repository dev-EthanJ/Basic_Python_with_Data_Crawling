{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c52a2a",
   "metadata": {},
   "source": [
    "## 순회 크롤러 (crawler with traversal)\n",
    "\n",
    "- 같은 양식의 페이지를 순회하면서 자료를 수집해오는 크롤러\n",
    "\n",
    "- 원 페이지 크롤러 제작 후 > 완성된 크롤러를 반복문에 넣어서 만든다\n",
    "\n",
    "> 반복을 어디부터 돌릴지에 대한 파악이 제일 중요!\n",
    "\n",
    "<br>\n",
    "\n",
    "- 순서\n",
    "\n",
    "1. approach N page\n",
    "\n",
    "2. source code crawling\n",
    "\n",
    "3. parsing\n",
    "\n",
    "4. data extraction\n",
    "\n",
    "5. saving in txt file\n",
    "\n",
    "6. move to number 1.\n",
    "\n",
    "> 다음페이지 버튼 XPATH 클릭으로 페이지 넘기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b306b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crwaling library import\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "\n",
    "# 코드 진행 지연을 위한 time 임포트\n",
    "import time\n",
    "\n",
    "# 2022-07 이후 selenium 업데이트로 인한 XPATH 추적 시 사용하는 임포트\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# file io\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12115495",
   "metadata": {},
   "source": [
    "- 리스트 형식 페이지: [F12] + [Network menu click] > 리스트 다음 페이지 클릭    \n",
    "\\>  url 바뀌지 않아도, Network 변경사항을 [Headers], [Payload] tab에서 확인 가능     \n",
    "\\> XPATH 구하기 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4ec077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916\n",
      "916\n",
      "916\n",
      "916\n"
     ]
    }
   ],
   "source": [
    "chrome_driver = webdriver.Chrome('chromedriver')\n",
    "\n",
    "# approach first page\n",
    "chrome_driver.get(\"https://product.kyobobook.co.kr/bestseller/online?period=001\")\n",
    "\n",
    "# 첫번째 제목 저장 리스트 > 반복문 중지 조건으로 필요\n",
    "check_name_list = list()\n",
    "\n",
    "rank_list = list()\n",
    "title_list = list()\n",
    "price_list = list()\n",
    "author_list = list()\n",
    "\n",
    "time.sleep(6)\n",
    "\n",
    "# 반복문\n",
    "while True:\n",
    "    \n",
    "    # 끝까지 스크롤 다운 (광고로 페이지 가리기 방지)\n",
    "    chrome_driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # source code crawling\n",
    "    source = chrome_driver.page_source\n",
    "    \n",
    "    # parsing\n",
    "    html_parsed_source = BeautifulSoup(source, \"html.parser\")\n",
    "\n",
    "    # extract data(span_prod_name) & saving in list\n",
    "    \n",
    "    ####### Title\n",
    "    span_prod_name = html_parsed_source.find_all(\"span\", class_=\"prod_name\")   \n",
    "    \n",
    "    # while문 중지 조건 -> 같은 title이 list에 존재 할 때\n",
    "    if (span_prod_name[0].text in check_name_list):\n",
    "        chrome_driver.close()\n",
    "        break\n",
    "    check_name_list.append(span_prod_name[0].text)\n",
    "    \n",
    "    for title in span_prod_name:\n",
    "        title_list.append(title.text)     \n",
    "    \n",
    "    ######## Rank\n",
    "    div_prod_rank = html_parsed_source.find_all(\"div\", class_=\"prod_rank\")\n",
    "\n",
    "    for rank in div_prod_rank:\n",
    "        rank_list.append(rank.text)\n",
    "\n",
    "    ######## Price        \n",
    "    span_val = html_parsed_source.find_all(\"span\", class_=\"val\")\n",
    "    \n",
    "    for price in span_val:\n",
    "        if(price.text == \"0\"):\n",
    "            None\n",
    "        else:\n",
    "            price_list.append(price.text)\n",
    "    \n",
    "    ######### Author\n",
    "    span_prod_author = html_parsed_source.find_all(\"span\", class_=\"prod_author\")\n",
    "\n",
    "    for author in span_prod_author:\n",
    "        author_list.append(author.text.split(\" ·\")[0])\n",
    "    \n",
    "    # 다음 페이지 버튼 XPATH로 이동\n",
    "    chrome_driver.find_element(By.XPATH, '//*[@id=\"tabRoot\"]/div[4]/div[2]/button[2]').click()\n",
    "    \n",
    "    time.sleep(6)\n",
    "\n",
    "# extracted data item 개수 일치하는 지 확인\n",
    "book_list = [ title_list, rank_list, price_list, author_list ]\n",
    "\n",
    "for book in book_list:\n",
    "    print(len(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b816736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv로 출력\n",
    "w_csv = codecs.open(\"C:/Users/EthanJ/develop/PLAYDATA/Python_basic/crawling/crawler_with_traversal.csv\", 'w', \"utf-8-sig\")\n",
    "\n",
    "for i in range(len(title_list)):\n",
    "    this_line = \"%s,%s,%s,%s\\n\" %(rank_list[i].replace(',', '，'), title_list[i].replace(',', '，'),\n",
    "                                  author_list[i].replace(',', '，'), price_list[i].replace(',', '，'))\n",
    "    w_csv.write(this_line)\n",
    "\n",
    "w_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dde3c",
   "metadata": {},
   "source": [
    "![crawler_with_traversal](https://github.com/insung-ethan-j/Basic_Python_with_Data/blob/624712a91912e2bca227fb6bd577289ad1f42ba4/img_source/1013_1.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98977bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
